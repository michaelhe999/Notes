\documentclass[10pt,letterpaper,unboxed,cm]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[margin=1in]{geometry}
\usepackage[normalem]{ulem}
\usepackage{latexsym}
\usepackage{xcolor}
\usepackage{color}
\usepackage{float}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{cleveref}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{minted}

\newcommand{\tab}{~~~~}

\begin{document}
\begin{center}
    \textbf{\Large{CSE 150A Notes}}
\end{center}
\section{9/26}
\subsection{Course Information}
Prerequisites: 
\begin{itemize}
    \item Programming Knowledge
    \item Elementary Probability
    \begin{itemize}
        \item Random Variables - discrete and continuous
        \item Expected Values - sums and integrals
    \end{itemize}
        \item Multivariable Calculus
        \item Linear Algebra
\end{itemize}
HW Released Tuesdays, due Monday
24 hr late policy for HW
Quizzes in person every Thursday lecture -- Based on HW
Point lost on quizzes go to the Final
Midterm: 10/31 (Week 5) in class
Final: 12/5 (Week 10) in class
One sheet of handwritten notes allowed
\subsection{Course Overview}
\begin{itemize}
    \item Inference and learning in Bayesian Networks
    \item Markov decision processes for reinforcement learning
\end{itemize}
Does not cover:
\begin{itemize}
    \item Neural architectures
    \item Purely logical reasoning
    \item Heuristic search (A*)
    \item Theorem proving
    \item Genetic algorithms
    \item Philosophy of AI
\end{itemize}
\section{10/1}
Probability Theory: how knowledge affects belief (Poole and Mackworth)\\
This view is known as the Bayesian view of probability\\
\tab Other view is the frequentist view; probability is the limit of the relative frequency of an event\\
Discrete Random Variables: denoted with capital letters\\
Domain of possible values for a variable, denoted with lowercase letters\\
Uncoditional (prior) probability: $P(X = x)$\\
Axioms of Probability:\\
\tab $P(X = x) \geq 0$\\
\tab $\sum_{i=1}^n P(X = x_i) = 1$\\
\tab $P(X = x_i \text{ or } X = x_j) = P(X = x_i) + P(X = x_j) \text{ iff } x_i \neq x_j$\\
Conditional Probability: $P(X = x_i | Y = y_j)$\\
In this case X and Y are dependent\\
Bayes rule: $P(X = x_i | Y = y_j) = \frac{P(Y = y_j | X = x_i)P(X = x_i)}{P(Y = y_j)}$\\
Product rule: $P(X = x_i, Y = y_j) = P(X = x_i | Y = y_j)P(Y = y_j) = P(x)P(y|x)$\\
Marginalization: $P(X = x_i) = \sum_{j=1}^n P(X = x_i, Y = y_j)$\\
Independence: $P(X = x_i, Y = y_j) = P(X = x_i)P(Y = y_j)$\\
\section{10/3}
Marginal Independence: $P(X = x_i, Y = y_j) = P(X = x_i)P(Y = y_j)$\\
 $P(X|Y) = P(X)$\\
  $P(Y|X) = P(Y)$\\
Conditional Independence: $P(X,Y|E) = P(X|E)P(Y|E)$\\
$P(X|Y,E) = P(X|E)$\\
$P(Y|X,E) = P(Y|E)$\\
Suppose $X_i \in \{0, 1\}$; then it requires $O(2^n)$ parameters to represent the joint distribution\\
\tab Goals: compact representation, efficient inference\\
Use belief to simplify the joint distribution\\
\tab Conditional Probability Tables (CPT) represent the conditional probability of a variable given its parents\\
\tab Any inference can be explained in terms of the joint probability, using product rule and marginalization\\
To perform inference efficiently:\\
\tab Visualize models as directed acyclic graphs (DAGs)\\
\tab Exploit the graph structure to simplify and organize calculations\\
Absent edges represent assumptions of independence\\
Visual representation of the joint distribution is called a Bayesian Network or belief network\\
\tab Nodes represent random variables\\
\tab Edges represent direct dependencies\\
\tab CPTs for each node describe how each node depends on its parents\\
Belief network = DAG + CPTs\\
It's always true from the product rule that $P(X_1, X_2, ..., X_n) = P(X_1)P(X_2|X_1)\cdots = \Sigma^n_{i=1}P(X_i|X_1, X_2, \ldots, X_{i-1})$\\
But suppose in a particular domain that $P(X_i|X_1, X_2, \ldots, X_{i-1}) = P(X_i|\text{parents}(X_i))$\\
Where parents is a subset of ${X_1, X_2, \ldots, X_{i-1}}$\\\\
To create a belief network: \\
Choose random variables of interest, choose ordering of the variables, and:\\
While there are variables left, add the node $X_i$ to the network, with parents the minimum subset satisfying:\\
$P(X_1, X_2 \ldots X_n) = \Sigma^n_{i=1}P(X_i|\text{parents}(X_i))$\\
Define CPTs\\
Best order is to take "root causes", then variables they influence, etc.\\
Edge does not necessarily represent dependence, especially if a bad order is chosen\\
DAGs encode qualitative knowledge: assumptions of marginal and conditional independence\\
CPTs encode quantitative knowledge: numerical influences of some variables on others\\
\section{10/8}

\end{document}
